# Process big data transaction stream in near real-time with Apache Flink

**Learn how to process IOTA transaction streams in near real-time, 
while diving into the big data stream processing framework Apache Flink.**

:::info:
The Tangle streaming libraries in this guide are not recommended for production environments.
Feel free to contribute to the libraries, so that they eventually become production ready.
:::

[Apache Flink](https://flink.apache.org/) is an open source stream processing framework.
This guide describes how you can connect Flink to the IOTA tangle.

## Requirements

- Operation system: Linux, MacOS, BSD or Windows
- Memory: 2GB
- Storage: 10GB free space

## Notice on Java

This guide uses the Scala programming language. If you consider yourself as beginner, you should stick to Scala.
If you want to use Scala in Java, you have to add the [Scala library to maven or sbt](https://mvnrepository.com/artifact/org.scala-lang/scala-library).
sbt can also be used with Java. [This guide](http://xerial.org/blog/2014/03/24/sbt/) might be useful to get sbt running along with Java. 
The [Artima guide](https://www.artima.com/pins1ed/combining-scala-and-java.html) describes how you can use Scala in Java.
If you want to use Java only, you have to implement the libraries by yourself.

## What data are available?

Since the [Flink tangle source library](https://github.com/Citrullin/flink-tangle-source) uses the ZMQ API, all
[ZMQ events](../references/zmq-events.md) are therefore also available in Flink.
The Flink streaming library uses the [ZeroMQMessageParser](https://github.com/Citrullin/tangle-streaming/blob/master/src/main/scala/org/iota/tangle/stream/ZeroMQMessageParser.scala)
from the [Tangle streaming library](https://github.com/Citrullin/tangle-streaming) to parse the raw event messages into class instances.
All ZMQ event messages are wrapped in classes generated by [protobuf schema files](https://github.com/Citrullin/tangle-streaming/tree/master/src/main/protobuf).
All protobuf defined messages and attributes are also available in Flink.

## Installation

### 1. Install Java >=8

**_Notice:_** Since Oracle Java is not free of charge for business environments anymore, 
you should consider to use OpenJDK instead.

Since Scala uses the jvm, we need to install Java 8 or higher.

- [OpenJDK Installation guide](http://openjdk.java.net/install/)
- [Oracle Java JDK Installation guide](https://docs.oracle.com/javase/8/docs/technotes/guides/install/install_overview.html)


### 2. Install sbt

Follow [this guide](https://www.scala-sbt.org/1.x/docs/Setup.html) to install sbt.

### 3. Build libraries

Since the libraries are not supposed to be used in production environments, you cannot find them in any official
repository. You have to build and publish them locally.
Clone the repositories:

**Tangle streaming:**
```bash
git clone https://github.com/Citrullin/tangle-streaming.git
```

**Flink Tangle source**
```bash
git clone https://github.com/Citrullin/flink-tangle-source
```

After cloning you have to build & publish the repositories:

```bash
cd tangle-streaming && sbt
```

After exeuting thit, the sbt repl initializes. That can take a moment.
When the initialization is done, execute the following command within in the repl.

```bash
compile
publishLocal
```

When you are done, press CTRL + C to terminate the repl.

```bash
cd ../flink-tangle-source && sbt
```

Same as before, after initialization, execute the following again.

```bash
compile
publishLocal
```

### 4. Add dependencies to build.sbt

```scala
libraryDependencies += "org.iota" %% "flink-tangle-source" % "0.0.1",
```


## Usage

**_Note:_** There are [some examples available](https://github.com/iota-community/flink-tangle-examples).

If you run your own IRI, you have to [enable and to configure the ZMQ API.](../references/iri-configuration-options.md)
[Tanglebeat provides a list of public ZMQ fullnodes.](http://tanglebeat.com/page/internals)
cIRI does not support the ZMQ API at the moment.

```scala
object App {
    def main(args: Array[String]): Unit = {
        val zeroMQHost = "HOSTNAME|IP"
        val zeroMQPort = 5556
        val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment
        val stream = env.addSource(new TangleSource(zeroMQHost, zeroMQPort, ""))
    }
}
```

### Use case: Most used addresses in the last hour (Top 10)

If you are not familiar with Flink, you should take a look into [this documentation first](https://ci.apache.org/projects/flink/flink-docs-master/tutorials/datastream_api.html#writing-a-flink-program). 

This guide describes step for step the example in MostUsedAddresses.scala. 
All examples are available [here](https://github.com/iota-community/flink-tangle-examples).

Let's start with the setup of the stream.

```scala
    val unconfirmedMessageDescriptorName = UnconfirmedTransactionMessage.scalaDescriptor.fullName
    val zeroMQHost = "HOSTNAME|IP"
    val zeroMQPort = config.getInt(ConfigurationKeys.ZeroMQ.port)
    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment

    val stream = env.addSource(new TangleSource(zeroMQHost, zeroMQPort, ""))
```

We simply get the ZMQ data stream by a defined hostname and port. We can also subscribe one specific topic
Since we are only using the UnconfirmedTransactionMessage, we could use [tx](../references/zmq-events.md#tx).

```scala
val stream = env.addSource(new TangleSource(zeroMQHost, zeroMQPort, "tx"))
```

Since we get a stream of GeneratedMessage, we need to filter with the [protobuf descriptor](https://developers.google.com/protocol-buffers/docs/reference/cpp/google.protobuf.descriptor). 

```scala
    val filteredStream = stream
      .filter(_.companion.scalaDescriptor.fullName == unconfirmedMessageDescriptorName)
```

We can make sure with this that the stream only contains UnconfirmedTransactionMessages.
So now we check the type, wrap it in an option and getting the value. 
Since we already filtered on the descriptor, we know that every event is of type UnconfirmedTransactionMessage.
If not, something fundamental is wrong and a NullPointerException will crash the application.

```scala
val unconfirmedTransactionStream = filteredStream.map(_ match {
        case m: UnconfirmedTransactionMessage => Some(m)
        case _ => None
      })
      .map(_.get)
```

This is a uncommon and dirty way to do. You should never use get, since you can run in NullPointerExceptions.
Use [getOrElse](https://www.tutorialspoint.com/scala/scala_options.htm) instead. 
It would also make sense to implement a filter into the library, so that the correct type is returned.
That would make the type checking obsolete.
Since this library is just a proof of concept, we go with this dirty solution for now.

Now we have our stream of the type UnconfirmedTransactionMessage. We basically get every message our full-node receives.
We want to find out which addresses were used the most. That means, we only need the address and some counter.
For simplicity we count every address in a transaction as one. We could also only keep the inputs. 
To detect double used addresses, we can also filter on outputs. 
If you want to do that, you have to apply a filter with value > 0 or value < 0.

```scala
val addressOnlyStream = unconfirmedTransactionStream.map(e => (e.address, 1L))
```

Simple as that. We change the structure of our element with this simple map function.
We only keep the address and a counter. [Tuples](https://docs.scala-lang.org/tour/tuples.html) are useful for this.

Since we want to count our elements, we can key our stream by the address. 
This gives us a KeyedStream partitioned by the address. For more complex use-cases you can use [windowAll](https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/operators/windows.html#window-assigners)

```scala
val keyedStream = addressOnlyStream.keyBy(_._1)
```
 
So, now we have a KeyedStream where every partition only contain some tuples of the same kind. 
Basically a lot of tuples with the same address and 1L.
Partitioning is useful if you want to process a huge amount of data.
Flink can execute the processor for each partition on different nodes in your cluster, so that the process functions
on each stream can work in parallel. Therefore you are able to scale horizontally.

Next, we need to calculate the number of transactions for each address within one hour. 
Sliding Windows are useful for this. An update interval of 30 seconds is fine for our use-case.

```scala
val keyedTimedWindow = keyedStream.timeWindow(Time.minutes(60), Time.seconds(30))
```

We got our keyedTimeWindows. Now we need to aggregate our partitions. 
We have two options for this. The simplest variant is the reduce function. 
This is a function which reduces all elements to the few we really need. 
In our case, this would be our reduce function:

```scala
val aggregatedKeyedTimeWindow = timedWindow.reduce((a, b) => (a._1, a._2 + b._2))
```

Simple as that. The other variant is an aggregation function. One example:

```scala
val aggregatedKeyedTimeWindow = keyedTimedWindow.aggregate(new AddressCountAggregator)
```

The AddressCountAggregator class

```scala
class AddressCountAggregator extends AggregateFunction[(String, Long), (String, Long), (String, Long)]
{
  override def add(value: (String, Long), accumulator: (String, Long)): (String, Long) =
    (value._1, value._2 + accumulator._2)

  override def createAccumulator(): (String, Long) = ("", 0L)

  override def getResult(accumulator: (String, Long)): (String, Long) = accumulator

  override def merge(a: (String, Long), b: (String, Long)): (String, Long) = (a._1, a._2 + b._2)
}
```

The reduce function is used whenever you just need to reduce your result. Sums are a good example. 
Therefore in our case the reduce function makes more sense than the aggregation function.
Aggregation functions are helpful when you have complex operations. 
You can find one more complex example in [BundleAggregation.scala](https://github.com/iota-community/flink-tangle-examples/blob/master/src/main/scala/org/iota/tangle/flink/examples/BundleAggregation.scala).
The BundleAggregation combines incoming transaction into a Bundle and split them into UnconfirmedBundles
and ReattachedUnconfirmedBundles. This example is a simplification and does not split the Bundles in an accurate way.

Next we want to aggregate all elements and want to find the top ten addresses.
The timeWindowAll functions returns a AllWindowedStream. So all elements are combined in one stream again. 
Since we used a SlidingWindow on our partitions before, the time here is not that important anymore. 
So, we just use one second.

```scala
val timeWindowAll = aggregatedKeyedTimeWindow
      .timeWindowAll(Time.seconds(1))
```

Our AllWindowedStream contains all reduced partitions in a tuple. 
Each partition has one tuple in the structure (ADDRESS, AMOUNT_OF_TRANSACTIONS).
The last step is to find out which addresses are used the most. So we use an aggregation function for this.

```scala
val mostUsedStream = timeWindowAll.aggregate(new MostUsedAddressesAggregator(10))
```

The MostUsedAddressesAggregator class

```scala
class MostUsedAddressesAggregator(number: Int) extends AggregateFunction[(String, Long), Map[String, Long], List[(String, Long)]]
{
  override def add(value: (String, Long), accumulator: Map[String, Long]): Map[String, Long] = {
    accumulator ++ Map(value._1 -> (value._2 + accumulator.getOrElse(value._1, 0L)))
  }

  override def createAccumulator(): Map[String, Long] = Map()

  override def getResult(accumulator: Map[String, Long]): List[(String, Long)] =
    accumulator.toList.sortWith(_._2 > _._2).take(number)

  override def merge(a: Map[String, Long], b: Map[String, Long]): Map[String, Long] = {
    val seq = a.toSeq ++ b.toSeq
    val grouped = seq.groupBy(_._1)
    val mapWithCounts = grouped.map{case (key, value) => (key, value.map(_._2))}

    mapWithCounts.map{case (key, value) => (key, value.sum)}
  }
}
```

We use a Map as accumulator. Maps are really useful, since they contain key value pairs. 
AggregateFunction returns a sorted List. From the top used address to the bottom one. 
We are only interested in the first ten, so we only take the first 10. 
The constructor of the class takes the number for it.

The last step is simple, print the List and execute our program.

```scala
mostUsedStream.print()

    // execute program
    env.execute("Most used addresses")
```

